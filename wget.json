[{"cmd":"wget","msg":"# Saves the HTML of a webpage to a particular file.\nwget -O bro.html http://bropages.org/","updated_at":"2016-02-10T16:39:56.000Z","id":107,"up":29,"down":2},{"cmd":"wget","msg":"# Download a file from a webserver and save to hard drive.\nwget http://www.openss7.org/repos/tarballs/strx25-0.9.2.1.tar.bz2","updated_at":"2015-05-16T20:33:22.000Z","id":205,"up":11,"down":0},{"cmd":"wget","msg":"# Download an entire website (more robust than wget -m)\nwget --random-wait -r -p -e robots=off -U mozilla http://www.example.com","updated_at":"2015-11-09T12:43:15.000Z","id":289,"up":10,"down":0},{"cmd":"wget","msg":"# Save file into directory\n# (set prefix for downloads)\nwget -P path/to/directory http://bropages.org/bro.html","updated_at":"2015-05-11T08:16:52.000Z","id":552,"up":4,"down":0},{"cmd":"wget","msg":"# wget example useful for automating a web call and appending results to a log\nwget -q --no-check-certificate -O - \"http://bropages.org\" \u003E\u003E ~/bropages.log","updated_at":"2014-10-31T17:54:55.000Z","id":174,"up":3,"down":1},{"cmd":"wget","msg":"# When downloading a huge file, it may become practical to pause and resume a download. Adding the option -c or --continue will resume an interrupted download.\nwget -c https://scans.io/data/umich/https/certificates/raw_certificates.csv.gz","updated_at":"2014-11-08T06:32:39.000Z","id":260,"up":3,"down":1},{"cmd":"wget","msg":"# Open tarball without downloading\nwget -qO - \"http://www.tarball.com/tarball.gz\" | tar zxvf -","updated_at":"2014-02-14T08:29:37.000Z","id":298,"up":3,"down":1},{"cmd":"wget","msg":"# Download a list of urls from a file \nwget -i urls.txt","updated_at":"2015-11-09T13:06:31.000Z","id":604,"up":2,"down":0},{"cmd":"wget","msg":"# Get your external ip address from icanhazip.com and echo to STDOUT\nwget -O - http://icanhazip.com/ | tail","updated_at":"2014-06-02T19:52:46.000Z","id":763,"up":2,"down":0},{"cmd":"wget","msg":"# (continue) downloading file from URL wich requires HTTP authentication\nwget -c --http-user=username --http-password=password http://auth.example.com/file.dat","updated_at":"2014-02-07T15:29:06.000Z","id":521,"up":1,"down":0},{"cmd":"wget","msg":"# Recursively download only files with the pdf extension upto two levels away\nwget -r -l 2 -A \"*.pdf\" http://papers.xtremepapers.com/CIE/Cambridge%20Checkpoint/","updated_at":"2014-07-05T05:44:45.000Z","id":818,"up":1,"down":0},{"cmd":"wget","msg":"# Download a web page or file, and name the resultant file what the remote server says it should be.\n# (Great for outfits like Sourceforge where the download link is a long, intractable string of characters)\nwget --content-disposition http://example.com/download.php?id=12345\u0026file=foobar.tgz\u0026datetime=20141004","updated_at":"2014-10-21T21:37:24.000Z","id":906,"up":1,"down":0},{"cmd":"wget","msg":"# print file to stdout like curl does\nwget -O - http://exmaple.com/text.txt","updated_at":"2015-11-09T12:46:10.000Z","id":1222,"up":1,"down":0},{"cmd":"wget","msg":"# Mirror an entire website\nwget -m http://google.com","updated_at":"2014-10-31T17:55:33.000Z","id":281,"up":2,"down":3}]